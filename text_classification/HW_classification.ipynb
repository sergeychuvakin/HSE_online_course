{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание по классификации текстов\n",
    "\n",
    "##  Задача: научиться строить классификационные алгоритмы на текстах\n",
    "\n",
    "### Подзадачи:\n",
    "\n",
    "- [в тетради приведён код] подготовить данные к классификации\n",
    "- [в тетради приведён код] обучить, оценить бейзлайновую модель Logit\n",
    "- реализовать модель, превосходящую по метрикам бейзлайновую модель Logit.\n",
    "- [в тетради приведён код] обучить, оценить бейзлайновую модель Gradient Boosting\n",
    "- реализовать модель, превосходящую по метрикам бейзлайновую модель Gradient Boosting.\n",
    "- обучить оценить модель CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "Для классификации вам будет предложен набор данных заклинаний из киновселенной \\<\\<Гарри Поттер\\>\\>. <br>\n",
    "\n",
    "Каждое заклинание - это текст определенной длины, который относится к определенному классу. <br> \n",
    "\n",
    "Для вашего удобства - всего будет **15** <ins>взвешенных</ins> классов. То есть - в каждом классе равное количество текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функции для проверки заданий\n",
    "# NB: вам в них вникать не нужно и менять тоже! они воспроизводят три файла, которые вам нужно будет загрузить на платформу\n",
    "\n",
    "def make_predictions(model):\n",
    "    import pickle\n",
    "    with open('vecs_logit.pickle', 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return model.predict(X)\n",
    "\n",
    "def make_predictions_CNN(model):\n",
    "    import pickle\n",
    "    with open('vecs_CNN.pickle', 'rb') as f: \n",
    "        vecs = torch.tensor(pickle.load(f))\n",
    "    vecs = vecs.to('cuda')\n",
    "\n",
    "    return torch.argmax(model(vecs), dim=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные\n",
    "!wget https://raw.githubusercontent.com/sergeychuvakin/random_scripts/master/harry_ech.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "SEED = 128 # для воспроизводимости результата, не менять!\n",
    "\n",
    "df = pd.read_csv('harry_ech.csv') # подгружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-----Класс-----|----Количество----|\n",
      "    STUPEFY\n",
      "                        500\n",
      "    ACCIO\n",
      "                        500\n",
      "    EXPELLIARMUS\n",
      "                        500\n",
      "    PROTEGO\n",
      "                        500\n",
      "    LUMOS\n",
      "                        500\n",
      "    CRUCIO\n",
      "                        500\n",
      "    AVADA_KEDAVRA\n",
      "                        500\n",
      "    SCOURGIFY\n",
      "                        500\n",
      "    INCENDIO\n",
      "                        500\n",
      "    IMPERIO\n",
      "                        500\n",
      "    ALOHOMORA\n",
      "                        500\n",
      "    EXPECTO_PATRONUM\n",
      "                        500\n",
      "    OBLIVIATE\n",
      "                        500\n",
      "    SECTUMSEMPRA\n",
      "                        500\n",
      "    LEGILIMENS\n",
      "                        500\n",
      "|---------------|------------------|\n"
     ]
    }
   ],
   "source": [
    "print('|-----Класс-----|----Количество----|')\n",
    "for i in Counter(df['_class']).items(): # проверяем распределение классов\n",
    "    print(f'    {i[0]}')\n",
    "    print(f'                        {i[1]}')\n",
    "print('|---------------|------------------|')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно классы взвешенные, следовательно можно больше внимания уделить именно моделированию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "В рамках этого домашнего задания мы предлагаем вам взять уже натренированную модель эмбедингов **fasttext** на википедии. Это достаточно удобно, когда нужно быстро построить хорошую модель, а ваши данные не сильно специфичны. Будем считать что заклинания из Гарри Поттера не сильно специфины для английского языка... <br>\n",
    "<br>\n",
    "Скачаем натреннированые эмбединги. Это может занять некоторое время, поэтому мы рекомендуем использовать [Google Colab]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если не установлен !pip install fasttext\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\n",
    "!unzip wiki.en.zip\n",
    "!rm -rf wiki.en.zip\n",
    "!rm -rf wiki.en.vec\n",
    "\n",
    "import fasttext\n",
    "model_ft = fasttext.load_model('wiki.en.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка текстов\n",
    "\n",
    "Мы здесь <ins>намерено</ins> пропускаем, детальную предобработку текстов, чтобы вы ее проделали сами, когда будете подбирать спецификацию для моделей. Но, напомним - что может в нее входить:\n",
    "- удаление пунктуации и служебных символов\n",
    "- токенизация\n",
    "- удаление стоп-слов \n",
    "- стемминиг/лемматизация \n",
    "- etc\n",
    "<br>\n",
    "\n",
    "Все это можено сделать множеством разных способов, выберите для себя наиболее удобный. <br>\n",
    "<br>\n",
    "\n",
    "Ниже мы рассмотрим вариант без предварительной обработки. <br> \n",
    "\n",
    "Мы помним, что fasttext возвращает вектора с заданной размеронстью для **слов**, но наша задача классифицировать **тексты**. Для этого нам необходимо из нескольких векторов сделать один для одного текста. Здесь можно пойти разными путями - просто суммировать, находить среднее, медиану, взвешивать вектора (к примеру по TF-IDF). Ниже мы просто усредним вектора, но вам необходмо будет взвесить их по TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# напишем отдельную функцию, которую применим к каждому из текстов.\n",
    "def get_vec(text):\n",
    "    tokens = [model_ft.get_word_vector(i) for i in text.split(' ')] # токенизация и возвращение матрицы для каждого текста\n",
    "    summed_vecs = reduce((lambda x,y: x+y), tokens) # суммированные тексты\n",
    "    return summed_vecs / len(text.split(' ')) # делим на количество слов в предложении \n",
    "\n",
    "X = df['text'].apply(get_vec).apply(pd.Series).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество текстов:  7500\n",
      "Размерность векторов:  (300,)\n"
     ]
    }
   ],
   "source": [
    "print('Количество текстов: ', X.shape[0])\n",
    "print('Размерность векторов: ', X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим переменную отклика\n",
    "# для этого мы просто пронуммеруем классы от 0 для 14\n",
    "\n",
    "rule = dict(zip(df['_class'].unique(), range(len(df['_class'].unique())))) #  правило подстановки\n",
    "\n",
    "y = df['_class'].replace(rule).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество меток:  7500\n"
     ]
    }
   ],
   "source": [
    "print('Количество меток: ', y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# разделим на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "# Самая простая модель с кроссвалидацией\n",
    "clf = LogisticRegressionCV(cv=2, max_iter=10000, solver='lbfgs', class_weight='balanced').fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32106666666666667\n",
      "Precision: 0.31364805622860986\n",
      "Recall: 0.32106666666666667\n",
      "F1-measure: 0.31469393834745363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "pred_y = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, pred_y))\n",
    "print('Precision:', precision_score(y_test, pred_y, average='weighted'))\n",
    "print('Recall:', recall_score(y_test, pred_y, average='weighted'))\n",
    "print('F1-measure:', f1_score(y_test, pred_y, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЗАДАЧА 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача изменить регрессию так, чтобы она побила по качеству (показала лучше метрики качества) ту, что мы разобрали выше. Вы в праве менять метапараметры, предобработку данных. Чем сильнее будет отличатся точность (accuracy) тем выша ваша оценка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "######### ВАШ КОД ЗДЕСЬ ##########\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы мы могли поставить вам оценку, необходимо чтобы вы сохранили модель в объект clf и воспроизвели чанк ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_submit_logit.txt', 'w') as f:\n",
    "    for i in make_predictions(clf):\n",
    "        f.write(str(i))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# возмем классификатор, с ошибкой softmax\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=15).fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2634666666666667\n",
      "Precision: 0.2641526496289002\n",
      "Recall: 0.2634666666666667\n",
      "F1-measure: 0.26074130351742697\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-measure:', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЗАДАЧА 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача изменить градиентный бустинг так, чтобы он побил по качеству (показала лучше метрики качества) тот, что мы разобрали выше. Вы в праве менять метапараметры, предобработку данных. Чем сильнее будет отличатся точность (accuracy) тем выша ваша оценка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "######### ВАШ КОД ЗДЕСЬ ##########\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы мы могли поставить вам оценку, необходимо чтобы вы сохранили модель в объект clf и воспроизвели чанк ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_submit_gbm.txt', 'w') as f:\n",
    "    for i in make_predictions(clf):\n",
    "        f.write(str(i))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN classifier (baseline)\n",
    "\n",
    "Для тренировки сверточной нейронной сети нам понадобится немного другая предобработка данных, поэтому мы проведем ее заново. Как и в прошлый раз, мы не заостряем внимание на базовых шагах обработки текста, но в то же время ожидаем ее от вас. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка текстов\n",
    "Перед определением самой модели, подготовим тексты специальным образом. Мы будем использовать фреймфорк PyTorch. Он в свою очередь ожидает на вход ряд подготовленных объектов:\n",
    "- Словарь токенов\n",
    "- Список двумерных матриц\n",
    "- Список меток\n",
    "- Модель эмбедингов\n",
    "\n",
    "В качестве модели эмбедингов возьмем fasttext, который использовали ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## инициализируем объекты\n",
    "texts = df['text']\n",
    "labels = df['_class']\n",
    "word2idx = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Матрицы, в которые мы превратим наши тексты должны быть одной размерности. Но как этого достичь? Дополним каждое предложение \"паддингами\" - специальными токенами, которые будут иметь один и тот же идентификатор. Кроме этого, создадим слово в словаре на случай модель столкнется со словом, которого не было в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx['<pad>'] = 0\n",
    "word2idx['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2 # первые два места в словаре уже заняты\n",
    "tokens = [] # здесь будем хранить токенизированные предложения \n",
    "max_len = 0 # создадим счетчик для посика максимальной длины\n",
    "\n",
    "## каждый текст в корпусе разделяем на токены и добавляем в словарь попутно считая максимальную длину\n",
    "for sent in texts:\n",
    "    _sent = word_tokenize(sent)\n",
    "    tokens.append(_sent)\n",
    "    max_len = max(len(_sent), max_len)\n",
    "    for word in _sent:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            \n",
    "## Создаем вектора из предложений. \n",
    "## При этом - все предложения дополняются падингами до максимальной длины, которую встречали в корпусе\n",
    "vecs = []\n",
    "for i in tokens:\n",
    "    i.extend(['<pad>'] * (max_len - len(i)))\n",
    "    vecs.append([word2idx[word] for word in i ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## создание словаря эмбедингов\n",
    "## для этого возьмем уже знакомую нам модель fasttext и заменим слова векторами \n",
    "emb = [model_ft.get_word_vector(i) for i in word2idx.keys()]\n",
    "emb = torch.tensor(emb) ## NB: обернуть объект в тензор "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## заменим метки классов на их номера от 0 до 14\n",
    "rule = dict(enumerate(set(labels)))\n",
    "rule = {ii:i for i, ii in rule.items()}\n",
    "\n",
    "labels = [rule[i] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более эфективного обучения переведем и для того, чтобы обучатся на батчах переведем наши тексты и метки в объект DataLoader. Кроме этого, добавим сэмплирование батчей в DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "##  обучающая выборка / тестовая выборка\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    vecs, labels, test_size=0.1, random_state=SEED)\n",
    "\n",
    "## перевод в тензоры     \n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "## создание объектов dataloader\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "train_sampler = RandomSampler(train_data) \n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "val_sampler = SequentialSampler(val_data) \n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь можем остановится с предобработкой и перейдем к созданию архитектуры самой сети. Мы намерено создали однослойную сеть, чтобы у вас была возможность ее улучшить. Архитектура сети уже дана, Вам же нужно написать метод forward и обучить нйронную сеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''Одномерная сверточная сеть'''\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 num_classes=len(set(labels)),\n",
    "                 dropout=0.5):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        ### Здесь добавлена возможность запуска без тренированных эмбедингов \n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        ### сверточный слой                                                \n",
    "        self.conv1d1 = nn.Conv1d(in_channels=self.embed_dim,\n",
    "                                 out_channels=100,\n",
    "                                 kernel_size=3) \n",
    "\n",
    "        # Fully-connected слой и dropout для регуляризации \n",
    "        self.fc = nn.Linear(100, num_classes) \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        ##################################\n",
    "        ######### ВАШ КОД ЗДЕСЬ ##########\n",
    "        ##################################\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение сверточной сети может занять много времени, поэтому мы вам рекомендуем пользоваться GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Само обучение довольно стандартно. Инициализируем модель, выбираем ошибку и оптимизатор. Для нашей задачи мы выбрали Кросс Энторопию и Адам в качестве оптимизатора. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ЗАДАЧА 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача обучить и попробовать специфицировать нейронноую сеть так, чтобы она была близкой или лучше логистической регрессии по критерию точности (accuracy). Вы в праве менять саму архитектуру (только это все равно дожна быть сверточная нейронная сеть, CNN), предобработку данных, а также процесс обучения. Чем выше точность (accuracy) тем выша ваша оценка.\n",
    "\n",
    "Hint: попробуйте не подавать на вход предобученные эмбединги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time \n",
    "import random\n",
    "\n",
    "model = CNN(pretrained_embedding=emb,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=len(word2idx),\n",
    "                 embed_dim=300,\n",
    "                 num_classes=len(set(labels)),\n",
    "                 dropout=0.5)\n",
    "\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(m.parameters(), lr=0.001)\n",
    "best_accuracy = 0\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch_i in range(EPOCHS):\n",
    "\n",
    "    pass # убрать во время написания кода\n",
    "\n",
    "    ##################################\n",
    "    ######### ВАШ КОД ЗДЕСЬ ##########\n",
    "    ##################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы мы могли поставить вам оценку, необходимо чтобы вы сохранили модель в объект model и воспроизвели чанк ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('to_submit_CNN.txt', 'w') as f:\n",
    "    for i in make_predictions_CNN(model):\n",
    "        f.write(str(i.item()))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python37764bitenvvenvcc7576f1113d4be8853b886fa765cb3a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
